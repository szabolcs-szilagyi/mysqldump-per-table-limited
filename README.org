#+TITLE: mysqldump per table, limited

* The why

Sometimes databases are just too big and a single dump just won't cut it.
Thinking here of those odd DBs that contain 200+ GBs of data with the odd 70 GB
tables. Those databases won't be backed up using a =mysqldump= script with a
cron... Just think about it: to download 70 GB of data via =mysqldump= takes a
long time, your query will timeout.
- But I can increase the execution time - You say
- Yes, but for how long? Also if you want a sane and consistent dump then you
  also need to lock - at least the table - all your clients will wait for an
  hour till the process runs?
- But then how should I do it?
- If you have such a nice database then you will have to take snapshots of the
  disk where the data resides OR have replication going on. But those are topics
  for your DBA team. So yeah, this script isn't for database maintenance, but
  for the lone developer who would like to work with somewhat realistic data.


* What will the script do?

- create one dump file per table in the database(s)
- if that table is large and there is a time column available, then it will try
  to limit the retrieved data to only get the latest records
- when the process times out multiple times, then it will try to get a dump with
  a simple limit

* What will you get?

A folder full of nice files that can be restored into your local mysql server,
so you can have some data for local development.

No, you won't get a sane and consistent dump, as we're limiting those pesky
large tables, so that you can get anything at all.

* Why not use mydumper?

While [[https://github.com/maxbube/mydumper][mydumber]] could be a good program (didn't use it myself), it will get you
all the data from the database. So you can get 100s of GBs, if that is okay for
you, then go for it. Personally I just need some data to work with, not all.

* Usage

Please see the help command.
